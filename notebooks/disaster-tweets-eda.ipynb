{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing required Libraries.","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport gensim\nimport string\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nfrom collections import  Counter\nstop=set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:12.506863Z","iopub.execute_input":"2022-06-20T01:54:12.507156Z","iopub.status.idle":"2022-06-20T01:54:19.441206Z","shell.execute_reply.started":"2022-06-20T01:54:12.507108Z","shell.execute_reply":"2022-06-20T01:54:19.440119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data and getting basic idea ","metadata":{}},{"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\n\nprint(tweet.head(3))\n\nprint(f'There are {tweet.shape[0]} rows and {tweet.shape[1]} columns in train')\nprint(f'There are {test.shape[0]} rows and {test.shape[1]} columns in train')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:19.444013Z","iopub.execute_input":"2022-06-20T01:54:19.444489Z","iopub.status.idle":"2022-06-20T01:54:19.523712Z","shell.execute_reply.started":"2022-06-20T01:54:19.444388Z","shell.execute_reply":"2022-06-20T01:54:19.521329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Class distribution","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\ntarget_counts = tweet.target.value_counts()\nfig = px.bar(target_counts, x=target_counts.index, y=target_counts)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:19.525218Z","iopub.execute_input":"2022-06-20T01:54:19.525533Z","iopub.status.idle":"2022-06-20T01:54:21.912924Z","shell.execute_reply.started":"2022-06-20T01:54:19.525485Z","shell.execute_reply":"2022-06-20T01:54:21.91206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Number of characters in tweets","metadata":{}},{"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfig = make_subplots(rows=1, cols=2)\nfig.add_trace(\n    go.Histogram(x=tweet[tweet['target']==1]['text'].str.len()),\n    row=1,\n    col=1\n)\nfig.add_trace(\n    go.Histogram(x=tweet[tweet['target']==0]['text'].str.len()),\n    row=1,\n    col=2\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:21.914389Z","iopub.execute_input":"2022-06-20T01:54:21.914929Z","iopub.status.idle":"2022-06-20T01:54:22.11864Z","shell.execute_reply.started":"2022-06-20T01:54:21.914877Z","shell.execute_reply":"2022-06-20T01:54:22.117778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Number of words in a tweet","metadata":{}},{"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfig = make_subplots(rows=1, cols=2)\nfig.add_trace(\n    go.Histogram(x=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))),\n    row=1,\n    col=1\n)\nfig.add_trace(\n    go.Histogram(x=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))),\n    row=1,\n    col=2\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:22.12129Z","iopub.execute_input":"2022-06-20T01:54:22.121618Z","iopub.status.idle":"2022-06-20T01:54:22.341167Z","shell.execute_reply.started":"2022-06-20T01:54:22.121559Z","shell.execute_reply":"2022-06-20T01:54:22.340433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Average word length in a tweet","metadata":{}},{"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfig = make_subplots(rows=1, cols=2)\nword = tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nfig.add_trace(\n    go.Histogram(x=word.map(lambda x: np.mean(x))),\n    row=1,\n    col=1\n)\nword = tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nfig.add_trace(\n    go.Histogram(x=word.map(lambda x: np.mean(x))),\n    row=1,\n    col=2\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:22.342668Z","iopub.execute_input":"2022-06-20T01:54:22.343198Z","iopub.status.idle":"2022-06-20T01:54:22.857074Z","shell.execute_reply.started":"2022-06-20T01:54:22.343148Z","shell.execute_reply":"2022-06-20T01:54:22.856235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:22.858196Z","iopub.execute_input":"2022-06-20T01:54:22.858488Z","iopub.status.idle":"2022-06-20T01:54:22.86498Z","shell.execute_reply.started":"2022-06-20T01:54:22.858426Z","shell.execute_reply":"2022-06-20T01:54:22.863969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Common stopwords in tweets","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:22.866672Z","iopub.execute_input":"2022-06-20T01:54:22.867418Z","iopub.status.idle":"2022-06-20T01:54:22.906685Z","shell.execute_reply.started":"2022-06-20T01:54:22.867365Z","shell.execute_reply":"2022-06-20T01:54:22.906092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nx,y=zip(*top)\nfig = px.bar(x=x,y=y)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:36.216796Z","iopub.execute_input":"2022-06-20T01:54:36.217081Z","iopub.status.idle":"2022-06-20T01:54:36.536035Z","shell.execute_reply.started":"2022-06-20T01:54:36.217034Z","shell.execute_reply":"2022-06-20T01:54:36.535372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now,we will analyze tweets with class 1.","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:39.914157Z","iopub.execute_input":"2022-06-20T01:54:39.914448Z","iopub.status.idle":"2022-06-20T01:54:39.945743Z","shell.execute_reply.started":"2022-06-20T01:54:39.914398Z","shell.execute_reply":"2022-06-20T01:54:39.944955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nx,y=zip(*top)\nfig = px.bar(x=x,y=y)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:42.432574Z","iopub.execute_input":"2022-06-20T01:54:42.432875Z","iopub.status.idle":"2022-06-20T01:54:42.747763Z","shell.execute_reply.started":"2022-06-20T01:54:42.432827Z","shell.execute_reply":"2022-06-20T01:54:42.747046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","metadata":{}},{"cell_type":"markdown","source":"#### Analyzing punctuations","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(1)\nspecial = string.punctuation\ndic=defaultdict(int)\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n\nimport plotly.express as px\nx,y=zip(*dic.items())\nfig = px.bar(x=x,y=y)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:46.172405Z","iopub.execute_input":"2022-06-20T01:54:46.17273Z","iopub.status.idle":"2022-06-20T01:54:46.670983Z","shell.execute_reply.started":"2022-06-20T01:54:46.172682Z","shell.execute_reply":"2022-06-20T01:54:46.670209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=create_corpus(0)\nspecial = string.punctuation\ndic=defaultdict(int)\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n\nimport plotly.express as px\nx,y=zip(*dic.items())\nfig = px.bar(x=x,y=y)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:50.159928Z","iopub.execute_input":"2022-06-20T01:54:50.160218Z","iopub.status.idle":"2022-06-20T01:54:50.492979Z","shell.execute_reply.started":"2022-06-20T01:54:50.160169Z","shell.execute_reply":"2022-06-20T01:54:50.491977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Common words","metadata":{}},{"cell_type":"code","source":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:54:53.66443Z","iopub.execute_input":"2022-06-20T01:54:53.664772Z","iopub.status.idle":"2022-06-20T01:54:53.688281Z","shell.execute_reply.started":"2022-06-20T01:54:53.664722Z","shell.execute_reply":"2022-06-20T01:54:53.68748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.bar(x=x, y=y)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:55:15.158429Z","iopub.execute_input":"2022-06-20T01:55:15.158835Z","iopub.status.idle":"2022-06-20T01:55:15.478158Z","shell.execute_reply.started":"2022-06-20T01:55:15.158771Z","shell.execute_reply":"2022-06-20T01:55:15.477246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### bigram analysis","metadata":{}},{"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:19:32.243131Z","iopub.execute_input":"2022-06-20T02:19:32.243435Z","iopub.status.idle":"2022-06-20T02:19:32.250414Z","shell.execute_reply.started":"2022-06-20T02:19:32.243382Z","shell.execute_reply":"2022-06-20T02:19:32.249347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:30]\nx,y=map(list,zip(*top_tweet_bigrams))\n\nimport plotly.express as px\nfig = px.bar(x=x,y=y)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:19:44.976358Z","iopub.execute_input":"2022-06-20T02:19:44.976674Z","iopub.status.idle":"2022-06-20T02:19:46.029421Z","shell.execute_reply.started":"2022-06-20T02:19:44.976622Z","shell.execute_reply":"2022-06-20T02:19:46.028769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning","metadata":{}},{"cell_type":"code","source":"df=pd.concat([tweet,test])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:25:53.423904Z","iopub.execute_input":"2022-06-20T02:25:53.424205Z","iopub.status.idle":"2022-06-20T02:25:53.439195Z","shell.execute_reply.started":"2022-06-20T02:25:53.424154Z","shell.execute_reply":"2022-06-20T02:25:53.438211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing urls","metadata":{}},{"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nexample=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"\nremove_URL(example)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:25:58.613509Z","iopub.execute_input":"2022-06-20T02:25:58.613815Z","iopub.status.idle":"2022-06-20T02:25:58.620762Z","shell.execute_reply.started":"2022-06-20T02:25:58.613763Z","shell.execute_reply":"2022-06-20T02:25:58.619609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:27:15.156606Z","iopub.execute_input":"2022-06-20T02:27:15.156906Z","iopub.status.idle":"2022-06-20T02:27:15.21425Z","shell.execute_reply.started":"2022-06-20T02:27:15.156855Z","shell.execute_reply":"2022-06-20T02:27:15.213578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing HTML tags","metadata":{}},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\nexample = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"\nprint(remove_html(example))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:27:19.274245Z","iopub.execute_input":"2022-06-20T02:27:19.274557Z","iopub.status.idle":"2022-06-20T02:27:19.282417Z","shell.execute_reply.started":"2022-06-20T02:27:19.274506Z","shell.execute_reply":"2022-06-20T02:27:19.279718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:28:01.594004Z","iopub.execute_input":"2022-06-20T02:28:01.594292Z","iopub.status.idle":"2022-06-20T02:28:01.61596Z","shell.execute_reply.started":"2022-06-20T02:28:01.594241Z","shell.execute_reply":"2022-06-20T02:28:01.615006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Romoving Emojis","metadata":{}},{"cell_type":"code","source":"# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake 😔😔\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:28:11.11455Z","iopub.execute_input":"2022-06-20T02:28:11.114879Z","iopub.status.idle":"2022-06-20T02:28:11.126925Z","shell.execute_reply.started":"2022-06-20T02:28:11.114829Z","shell.execute_reply":"2022-06-20T02:28:11.125974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:28:26.096774Z","iopub.execute_input":"2022-06-20T02:28:26.097069Z","iopub.status.idle":"2022-06-20T02:28:26.177433Z","shell.execute_reply.started":"2022-06-20T02:28:26.09702Z","shell.execute_reply":"2022-06-20T02:28:26.17665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing punctuations","metadata":{}},{"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:28:35.950725Z","iopub.execute_input":"2022-06-20T02:28:35.951115Z","iopub.status.idle":"2022-06-20T02:28:35.958194Z","shell.execute_reply.started":"2022-06-20T02:28:35.951042Z","shell.execute_reply":"2022-06-20T02:28:35.95615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:28:37.588076Z","iopub.execute_input":"2022-06-20T02:28:37.588364Z","iopub.status.idle":"2022-06-20T02:28:37.651792Z","shell.execute_reply.started":"2022-06-20T02:28:37.588314Z","shell.execute_reply":"2022-06-20T02:28:37.651124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Spelling Correction\n","metadata":{}},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T02:28:59.689529Z","iopub.execute_input":"2022-06-20T02:28:59.689841Z","iopub.status.idle":"2022-06-20T02:29:07.731957Z","shell.execute_reply.started":"2022-06-20T02:28:59.689782Z","shell.execute_reply":"2022-06-20T02:29:07.730965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:29:47.864099Z","iopub.execute_input":"2022-06-20T02:29:47.864419Z","iopub.status.idle":"2022-06-20T02:29:48.017221Z","shell.execute_reply.started":"2022-06-20T02:29:47.864365Z","shell.execute_reply":"2022-06-20T02:29:48.016488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GloVe for Vectorization","metadata":{}},{"cell_type":"markdown","source":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","metadata":{}},{"cell_type":"code","source":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus=create_corpus(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:01.925315Z","iopub.execute_input":"2022-06-20T02:45:01.925631Z","iopub.status.idle":"2022-06-20T02:45:04.232637Z","shell.execute_reply.started":"2022-06-20T02:45:01.925579Z","shell.execute_reply":"2022-06-20T02:45:04.231824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:16.529442Z","iopub.execute_input":"2022-06-20T02:45:16.529848Z","iopub.status.idle":"2022-06-20T02:45:34.455624Z","shell.execute_reply.started":"2022-06-20T02:45:16.529799Z","shell.execute_reply":"2022-06-20T02:45:34.454791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model","metadata":{}},{"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making our submission","metadata":{}},{"cell_type":"code","source":"sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}